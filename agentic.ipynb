{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-agent-openai\n",
      "  Downloading llama_index_agent_openai-0.4.2-py3-none-any.whl.metadata (727 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.11 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-agent-openai) (0.12.11)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index-agent-openai)\n",
      "  Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: openai>=1.14.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-agent-openai) (1.59.8)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2.10.5)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.17.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.14.0->llama-index-agent-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (0.14.0)\n",
      "Requirement already satisfied: click in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (3.25.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.11->llama-index-agent-openai) (24.2)\n",
      "Downloading llama_index_agent_openai-0.4.2-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_llms_openai-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: llama-index-llms-openai, llama-index-agent-openai\n",
      "Successfully installed llama-index-agent-openai-0.4.2 llama-index-llms-openai-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-index-vector-stores-pinecone\n",
      "  Downloading llama_index_vector_stores_pinecone-0.4.2-py3-none-any.whl.metadata (716 bytes)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-vector-stores-pinecone) (0.12.11)\n",
      "Collecting pinecone-client<6.0.0,>=3.2.2 (from llama-index-vector-stores-pinecone)\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (2.10.5)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.17.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pinecone-client<6.0.0,>=3.2.2->llama-index-vector-stores-pinecone) (2024.12.14)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client<6.0.0,>=3.2.2->llama-index-vector-stores-pinecone)\n",
      "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client<6.0.0,>=3.2.2->llama-index-vector-stores-pinecone)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pinecone-client<6.0.0,>=3.2.2->llama-index-vector-stores-pinecone) (2.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.18.3)\n",
      "Requirement already satisfied: click in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.25.1)\n",
      "Requirement already satisfied: anyio in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (24.2)\n",
      "Downloading llama_index_vector_stores_pinecone-0.4.2-py3-none-any.whl (7.7 kB)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client, llama-index-vector-stores-pinecone\n",
      "Successfully installed llama-index-vector-stores-pinecone-0.4.2 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-index-readers-wikipedia\n",
      "  Downloading llama_index_readers_wikipedia-0.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-readers-wikipedia) (0.12.11)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2.10.5)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.18.3)\n",
      "Requirement already satisfied: click in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (3.25.1)\n",
      "Requirement already satisfied: anyio in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-wikipedia) (24.2)\n",
      "Downloading llama_index_readers_wikipedia-0.3.0-py3-none-any.whl (2.7 kB)\n",
      "Installing collected packages: llama-index-readers-wikipedia\n",
      "Successfully installed llama-index-readers-wikipedia-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-llms-openai in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.4 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-llms-openai) (0.12.11)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-llms-openai) (1.59.8)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2.10.5)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2.32.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.17.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->llama-index-llms-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (0.14.0)\n",
      "Requirement already satisfied: click in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (3.25.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/bianca/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-openai) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-agent-openai\n",
    "%pip install llama-index-vector-stores-pinecone\n",
    "%pip install llama-index-readers-wikipedia\n",
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sapore del Dune.pdf</td>\n",
       "      <td>Ristorante \"Sapore del Dune\"\\nChef Alessandra ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Universo Gastronomico di Namecc.pdf</td>\n",
       "      <td>Universo Gastronomico di Namecc\\nChef Alice Qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L Equilibrio Quantico.pdf</td>\n",
       "      <td>Ristorante \"L'Equilibrio Quantico\"\\nChef Aless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L Architetto dell Universo.pdf</td>\n",
       "      <td>Ristorante \"L'Architetto dell'Universo\"\\nChef ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L Essenza Cosmica.pdf</td>\n",
       "      <td>Ristorante \"L'Essenza Cosmica\"\\n\\nChef Alessan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stelle Astrofisiche.pdf</td>\n",
       "      <td>Ristorante \"Stelle Astrofisiche\"\\nChef Alessan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L Essenza di Asgard.pdf</td>\n",
       "      <td>Ristorante: L'Essenza di Asgard\\nChef Palissan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Eco di Pandora.pdf</td>\n",
       "      <td>Ristorante \"L'Eco di Pandora\"\\nChef Alessandra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L Eco dei Sapori.pdf</td>\n",
       "      <td>L'Eco dei Sapori\\nChef Aurora Vessanti\\n\\nNell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L Essenza delle Dune.pdf</td>\n",
       "      <td>Ristorante \"L'Essenza delle Dune\"\\nChef Alessa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L Universo in Cucina.pdf</td>\n",
       "      <td>Ristorante \"L'Universo in Cucina\"\\nChef Crina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Cosmica Essenza.pdf</td>\n",
       "      <td>Cosmica Essenza\\nAlla guida dello Chef Aurelio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L Oasi delle Dune Stellari.pdf</td>\n",
       "      <td>Ristorante \"L'Oasi delle Dune Stellari\"\\nChef:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Le Stelle Danzanti.pdf</td>\n",
       "      <td>Ristorante \"Le Stelle Danzanti\"\\nChef Asteria ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sala del Valhalla.pdf</td>\n",
       "      <td>Sala del Valhalla\\nBanchetto Asgardiano di Ecc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stelle dell Infinito Celestiale.pdf</td>\n",
       "      <td>Ristorante \"Stelle dell'Infinito Celestiale\"\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ristorante Quantico.pdf</td>\n",
       "      <td>Ristorante Quantico\\nChef Alessandra Temporini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Le Dimensioni del Gusto.pdf</td>\n",
       "      <td>Ristorante \"Le Dimensioni del Gusto\"\\n        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Datapizza.pdf</td>\n",
       "      <td>Ristorante \"L'Infinito Sapore\"\\nViaggio nel Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tutti a TARSvola.pdf</td>\n",
       "      <td>Ristorante \"Tutti a TARSvola\"\\nChef Executive:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Armonia Universale.pdf</td>\n",
       "      <td>Ristorante \"Armonia Universale\"\\nChef: Maestro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Le Stelle che Ballano.pdf</td>\n",
       "      <td>Ristorante \"Le Stelle che Ballano\"\\nChef Aless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>L Etere del Gusto.pdf</td>\n",
       "      <td>Ristorante \"L'Etere del Gusto\"\\nChef Executivo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Essenza dell Infinito.pdf</td>\n",
       "      <td>Ristorante \"L'Essenza dell'Infinito\"\\nChef Lun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Anima Cosmica.pdf</td>\n",
       "      <td>Ristorante \"Anima Cosmica\"\\nChef Aurora Stella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Eredita Galattica.pdf</td>\n",
       "      <td>L'Eredità Galattica\\nHub Temporale della Gastr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Il Firmamento.pdf</td>\n",
       "      <td>Il Firmamento\\nChef: Maestro Alessio \"Eterno\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>L infinito in un Boccone.pdf</td>\n",
       "      <td>Ristorante \"L'Infinito in un Boccone\"\\n\\nChef ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>L Essenza del Multiverso su Pandora.pdf</td>\n",
       "      <td>L'Essenza del Multiverso su Pandora\\nChef Matt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ristorante delle Dune Stellari.pdf</td>\n",
       "      <td>Il Ristorante delle Dune Stellari\\nChef Alessa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   FileName  \\\n",
       "0                       Sapore del Dune.pdf   \n",
       "1       Universo Gastronomico di Namecc.pdf   \n",
       "2                 L Equilibrio Quantico.pdf   \n",
       "3            L Architetto dell Universo.pdf   \n",
       "4                     L Essenza Cosmica.pdf   \n",
       "5                   Stelle Astrofisiche.pdf   \n",
       "6                   L Essenza di Asgard.pdf   \n",
       "7                        Eco di Pandora.pdf   \n",
       "8                      L Eco dei Sapori.pdf   \n",
       "9                  L Essenza delle Dune.pdf   \n",
       "10                 L Universo in Cucina.pdf   \n",
       "11                      Cosmica Essenza.pdf   \n",
       "12           L Oasi delle Dune Stellari.pdf   \n",
       "13                   Le Stelle Danzanti.pdf   \n",
       "14                    Sala del Valhalla.pdf   \n",
       "15      Stelle dell Infinito Celestiale.pdf   \n",
       "16                  Ristorante Quantico.pdf   \n",
       "17              Le Dimensioni del Gusto.pdf   \n",
       "18                            Datapizza.pdf   \n",
       "19                     Tutti a TARSvola.pdf   \n",
       "20                   Armonia Universale.pdf   \n",
       "21                Le Stelle che Ballano.pdf   \n",
       "22                    L Etere del Gusto.pdf   \n",
       "23                Essenza dell Infinito.pdf   \n",
       "24                        Anima Cosmica.pdf   \n",
       "25                    Eredita Galattica.pdf   \n",
       "26                        Il Firmamento.pdf   \n",
       "27             L infinito in un Boccone.pdf   \n",
       "28  L Essenza del Multiverso su Pandora.pdf   \n",
       "29       Ristorante delle Dune Stellari.pdf   \n",
       "\n",
       "                                                 Text  \n",
       "0   Ristorante \"Sapore del Dune\"\\nChef Alessandra ...  \n",
       "1   Universo Gastronomico di Namecc\\nChef Alice Qu...  \n",
       "2   Ristorante \"L'Equilibrio Quantico\"\\nChef Aless...  \n",
       "3   Ristorante \"L'Architetto dell'Universo\"\\nChef ...  \n",
       "4   Ristorante \"L'Essenza Cosmica\"\\n\\nChef Alessan...  \n",
       "5   Ristorante \"Stelle Astrofisiche\"\\nChef Alessan...  \n",
       "6   Ristorante: L'Essenza di Asgard\\nChef Palissan...  \n",
       "7   Ristorante \"L'Eco di Pandora\"\\nChef Alessandra...  \n",
       "8   L'Eco dei Sapori\\nChef Aurora Vessanti\\n\\nNell...  \n",
       "9   Ristorante \"L'Essenza delle Dune\"\\nChef Alessa...  \n",
       "10  Ristorante \"L'Universo in Cucina\"\\nChef Crina ...  \n",
       "11  Cosmica Essenza\\nAlla guida dello Chef Aurelio...  \n",
       "12  Ristorante \"L'Oasi delle Dune Stellari\"\\nChef:...  \n",
       "13  Ristorante \"Le Stelle Danzanti\"\\nChef Asteria ...  \n",
       "14  Sala del Valhalla\\nBanchetto Asgardiano di Ecc...  \n",
       "15  Ristorante \"Stelle dell'Infinito Celestiale\"\\n...  \n",
       "16  Ristorante Quantico\\nChef Alessandra Temporini...  \n",
       "17  Ristorante \"Le Dimensioni del Gusto\"\\n        ...  \n",
       "18  Ristorante \"L'Infinito Sapore\"\\nViaggio nel Te...  \n",
       "19  Ristorante \"Tutti a TARSvola\"\\nChef Executive:...  \n",
       "20  Ristorante \"Armonia Universale\"\\nChef: Maestro...  \n",
       "21  Ristorante \"Le Stelle che Ballano\"\\nChef Aless...  \n",
       "22  Ristorante \"L'Etere del Gusto\"\\nChef Executivo...  \n",
       "23  Ristorante \"L'Essenza dell'Infinito\"\\nChef Lun...  \n",
       "24  Ristorante \"Anima Cosmica\"\\nChef Aurora Stella...  \n",
       "25  L'Eredità Galattica\\nHub Temporale della Gastr...  \n",
       "26  Il Firmamento\\nChef: Maestro Alessio \"Eterno\" ...  \n",
       "27  Ristorante \"L'Infinito in un Boccone\"\\n\\nChef ...  \n",
       "28  L'Essenza del Multiverso su Pandora\\nChef Matt...  \n",
       "29  Il Ristorante delle Dune Stellari\\nChef Alessa...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded with 30 rows and 2 columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define the file path\n",
    "csv_file_path = './output/menus.csv'\n",
    "\n",
    "# Read the CSV file and assign headers explicitly\n",
    "df = pd.read_csv(csv_file_path, header=None, names=[\"FileName\", \"Text\"])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "display(df)\n",
    "\n",
    "# Confirm the data has been successfully loaded\n",
    "print(f\"DataFrame loaded with {len(df)} rows and {len(df.columns)} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'FileName': 'Sapore del Dune.pdf'}, page_content='Ristorante \"Sapore del Dune\"\\nChef Alessandra Quanti\\n\\nNel cuore arido di Tatooine, dove i mondi si mescolano e le stelle guidano i viaggiatori intergalattici, Chef\\nAlessandra Quanti porta una rivoluzione culinaria che sfida le distanze siderali. Non è raro vedere i\\ncommensali rimanere incantati osservando i suoi piatti che sembrano danzare tra le dune e le stelle, frutto\\ndella sua straordinaria padronanza degli stati quantici, che le permette di esplorare e materializzare le infinite\\npossibilità nascoste in ogni ingrediente rarefatto del deserto.\\n\\nLa sua storia ebbe inizio nei laboratori di spezie di Mos Eisley, dove la passione per la gastronomia\\nmolecolare si fuse con la sua profonda comprensione dell\\'universo subatomico. Fu proprio durante un\\nesperimento particolarmente intenso con i Cristalli Kyber che scoprì la sua innata capacità di percepire le\\nprobabilità culinarie, un dono che trasformò ogni sua creazione in un\\'esperienza di perfezione matematica\\nanche tra le tempeste di sabbia.\\n\\nLe sue presentazioni sono vere opere d\\'arte viventi, dove la luce danza e si trasforma, creando atmosfere\\nche portano gli ospiti in un viaggio tra le galassie. I commensali più attenti potrebbero notare come alcuni\\nelementi dei suoi piatti sembrino sfidare gentilmente la gravità, un\\'abilità raffinata durante il suo periodo\\ncome chef a bordo di uno Star Destroyer, dove la cucina assumeva significati completamente nuovi.\\n\\nUn giorno, un peculiare incidente con un forno quantico di sua invenzione le donò una comprensione unica\\ndel tempo culinario. Questo \"fortunato incidente\", come ama definirlo, le permise di perfezionare l\\'arte della\\ncottura a livelli inimmaginabili, orchestrando ogni secondo in una danza perfetta di temperature, anche nei\\nclimi più extraterritoriali.\\n\\nAl Sapore del Dune, ogni serata è un viaggio attraverso le infinite possibilità del gusto. Chef Quanti non si\\nlimita a cucinare: plasma la realtà stessa del cibo, trasformando ogni pasto in un\\'esperienza che trascende i\\nconfini tradizionali della gastronomia. Come ama ricordare ai suoi ospiti: \"In cucina, come nell\\'universo, tutto\\nè possibile. Dobbiamo solo saper osservare tutte le possibilità simultaneamente, per poi scegliere la più\\nstraordinaria.\"\\n\\nLe skill certificate dello Chef sono:\\n\\n           Psionica II\\n           Temporale I\\n           Gravitazionale I\\n           Quantistica 13\\n           Luce II\\n\\nIl ristorante possiede una tecnologia LTK II\\n\\nMenu\\nSinfonia Quantistica delle Stelle\\nIngredienti\\n           Shard di Prisma Stellare\\n           Lattuga Namecciana\\n           Radici di SingolaritàFibra di Sintetex\\n           Carne di Balena spaziale\\n           Teste di Idra\\n           Nettare di Sirena\\n           Sale Temporale\\n\\nTecniche\\n           Marinatura Temporale Sincronizzata\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Cottura a Vapore Termocinetica Multipla\\n           Cottura a Vapore Ecodinamico Bilanciato\\n\\nSinfonia Quantistica dell\\'Universo\\nIngredienti\\n           Shard di Prisma Stellare\\n           Foglie di Nebulosa\\n           Lattuga Namecciana\\n           Teste di Idra\\n           Carne di Mucca\\n           Farina di Nettuno\\n           Riso di Cassandra\\n           Fusilli del Vento\\n           Nduja Fritta Tanto\\n\\nTecniche\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Cottura a Vapore Termocinetica Multipla\\n           Marinatura Temporale Sincronizzata\\n\\nEvanescenza Quantica\\nIngredienti\\n           Cristalli di Memoria\\n           Radici di Singolarità\\n           Lattuga Namecciana\\n           Baccacedro\\n           Granuli di Nebbia Arcobaleno\\n           Plasma Vitale\\n           Petali di Eco\\n\\nTecniche\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Grigliatura Eletro-Molecolare a Spaziatura Variabile\\n           Bollitura Termografica a Rotazione Veloce\\n           Affumicatura a Stratificazione Quantica\\n           Saltare in Padella ClassicaGalassia di Sapori Sublimi\\nIngredienti\\n           Frammenti di Supernova\\n           Baccacedro\\n           Foglie di Nebulosa\\n           Radici di Singolarità\\n           Amido di Stellarion\\n           Fusilli del Vento\\n\\nTecniche\\n           Cottura con Microonde Entropiche Sincronizzate\\n           Congelamento Bio-Luminiscente Sincronico\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Marinatura Temporale Sincronizzata\\n           Impasto Gravitazionale Vorticoso\\n\\nGalassia di Sapore Quantico\\nIngredienti\\n           Polvere di Stelle\\n           Alghe Bioluminescenti\\n           Colonia di Mycoflora\\n           Carne di Drago\\n           Teste di Idra\\n           Plasma Vitale\\n           Nduja Fritta Tanto\\n\\nTecniche\\n           Grigliatura Eletro-Molecolare a Spaziatura Variabile\\n           Cottura a Vapore Termocinetica Multipla\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Impasto Gravitazionale Vorticoso\\n\\nInterstellare Risveglio di Kraken\\nIngredienti\\n           Polvere di Pulsar\\n           Liane di Plasmodio\\n           Radici di Singolarità\\n           Carne di Kraken\\n           Amido di Stellarion\\n           Riso di Cassandra\\n           Plasma Vitale\\n           Nduja Fritta TantoTecniche\\n          Congelamento Bio-Luminiscente Sincronico\\n          Affumicatura a Stratificazione Quantica\\n          Bollitura Infrasonica Armonizzata\\n\\nSinfonia Temporale delle Profondità Infrasoniche\\nIngredienti\\n          Polvere di Crononite\\n          Carne di Kraken\\n          Carne di Balena spaziale\\n          Funghi Orbitali\\n          Sale Temporale\\n\\nTecniche\\n          Saltare in Padella Classica\\n          Bollitura Infrasonica Armonizzata\\n          Cottura Sottovuoto Frugale Energeticamente Negativa\\n\\nPioggia di Dimensioni Galattiche\\nIngredienti\\n          Shard di Prisma Stellare\\n          Lattuga Namecciana\\n          Biscotti della Galassia\\n          Pane degli Abissi\\n          Nettare di Sirena\\n          Essenza di Vuoto\\n\\nTecniche\\n          Bollitura Termografica a Rotazione Veloce\\n          Affumicatura a Stratificazione Quantica\\n          Fermentazione Quantico Biometrica\\n\\nOde Cosmica di Terra e Stelle\\nIngredienti\\n          Polvere di Stelle\\n          Alghe Bioluminescenti\\n          Teste di Idra\\n          Fibra di Sintetex\\n          Carne di Balena spaziale\\n          Spaghi del Sole\\n          Nettare di Sirena\\n          Spezie MelangeTecniche\\n            Grigliatura Eletro-Molecolare a Spaziatura Variabile\\n            Marinatura Temporale Sincronizzata\\n\\nSinfonia Galattica\\nIngredienti\\n            Cristalli di Nebulite\\n            Colonia di Mycoflora\\n            Teste di Idra\\n            Carne di Xenodonte\\n            Carne di Drago\\n            Pane degli Abissi\\n            Funghi dell’Etere\\n\\nTecniche\\n            Cottura a Vapore con Flusso di Particelle Isoarmoniche\\n            Marinatura Temporale Sincronizzata')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"Text\")\n",
    "doc_data = loader.load()\n",
    "doc_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data/Menu\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key=\"AnzfgthfcrfRzttoXGiKZUJDMRlcB3w4uemf0PJGFFT5\"\n",
    ")\n",
    "\n",
    "OVERWRITE = False\n",
    "\n",
    "model = ModelInference(\n",
    "    model_id=\"mistralai/mistral-large\",  # Che conosciamo bene 😊🏆\n",
    "    credentials=credentials,\n",
    "    project_id=\"5c33debb-5a25-4bfe-8392-ede4b20884fe\",\n",
    "    params={\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_access_token' with your actual Hugging Face token\n",
    "access_token = \"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\"\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "\n",
    "# Transform a string into input zephyr-specific input\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "\n",
    "# Transform a list of chat messages into zephyr-specific input\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "    # ensure we start with a system prompt, insert blank if needed\n",
    "    if not prompt.startswith(\"<|system|>\\n\"):\n",
    "        prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "    # add final assistant prompt\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B\",\n",
    "    tokenizer_name=\"meta-llama/Llama-3.2-1B\",\n",
    "    context_window=3000,\n",
    "    max_new_tokens=512,\n",
    "    generate_kwargs={\"temperature\": 0, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core import SummaryIndex\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "\n",
    "# initialize settings (set chunk size)\n",
    "Settings.llm = OpenAI()\n",
    "Settings.chunk_size = 1024\n",
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "# Define Summary Index and Vector Index over Same Data\n",
    "summary_index = SummaryIndex(nodes, storage_context=storage_context)\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "# define query engines\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    name=\"summary_tool\",\n",
    "    description=(\n",
    "        \"Riassumi il contesto relativo ai ristoranti della galassia, tenendo separate le spiegazioni di ciascun piatto, con relativi ingredienti e tecniche\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    name=\"vector_tool\",\n",
    "    description=(\n",
    "        \"Estrai documenti specifici per rispondere alla domanda sui piatti da mangiare nella galassia, date le specifiche della domanda dell'utente\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.retrievers import EnsembleRetriever, TFIDFRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'FileName': 'Sapore del Dune.pdf'}, page_content='Ristorante \"Sapore del Dune\"\\nChef Alessandra Quanti\\n\\nNel cuore arido di Tatooine, dove i mondi si mescolano e le stelle guidano i viaggiatori intergalattici, Chef\\nAlessandra Quanti porta una rivoluzione culinaria che sfida le distanze siderali. Non è raro vedere i\\ncommensali rimanere incantati osservando i suoi piatti che sembrano danzare tra le dune e le stelle, frutto\\ndella sua straordinaria padronanza degli stati quantici, che le permette di esplorare e materializzare le infinite\\npossibilità nascoste in ogni ingrediente rarefatto del deserto.'),\n",
      " Document(metadata={'FileName': 'Sapore del Dune.pdf'}, page_content=\"La sua storia ebbe inizio nei laboratori di spezie di Mos Eisley, dove la passione per la gastronomia\\nmolecolare si fuse con la sua profonda comprensione dell'universo subatomico. Fu proprio durante un\\nesperimento particolarmente intenso con i Cristalli Kyber che scoprì la sua innata capacità di percepire le\\nprobabilità culinarie, un dono che trasformò ogni sua creazione in un'esperienza di perfezione matematica\\nanche tra le tempeste di sabbia.\\n\\nLe sue presentazioni sono vere opere d'arte viventi, dove la luce danza e si trasforma, creando atmosfere\\nche portano gli ospiti in un viaggio tra le galassie. I commensali più attenti potrebbero notare come alcuni\\nelementi dei suoi piatti sembrino sfidare gentilmente la gravità, un'abilità raffinata durante il suo periodo\\ncome chef a bordo di uno Star Destroyer, dove la cucina assumeva significati completamente nuovi.\"),\n",
      " Document(metadata={'FileName': 'Sapore del Dune.pdf'}, page_content='Un giorno, un peculiare incidente con un forno quantico di sua invenzione le donò una comprensione unica\\ndel tempo culinario. Questo \"fortunato incidente\", come ama definirlo, le permise di perfezionare l\\'arte della\\ncottura a livelli inimmaginabili, orchestrando ogni secondo in una danza perfetta di temperature, anche nei\\nclimi più extraterritoriali.\\n\\nAl Sapore del Dune, ogni serata è un viaggio attraverso le infinite possibilità del gusto. Chef Quanti non si\\nlimita a cucinare: plasma la realtà stessa del cibo, trasformando ogni pasto in un\\'esperienza che trascende i\\nconfini tradizionali della gastronomia. Come ama ricordare ai suoi ospiti: \"In cucina, come nell\\'universo, tutto\\nè possibile. Dobbiamo solo saper osservare tutte le possibilità simultaneamente, per poi scegliere la più\\nstraordinaria.\"\\n\\nLe skill certificate dello Chef sono:\\n\\n           Psionica II\\n           Temporale I\\n           Gravitazionale I\\n           Quantistica 13\\n           Luce II'),\n",
      " Document(metadata={'FileName': 'Sapore del Dune.pdf'}, page_content='Le skill certificate dello Chef sono:\\n\\n           Psionica II\\n           Temporale I\\n           Gravitazionale I\\n           Quantistica 13\\n           Luce II\\n\\nIl ristorante possiede una tecnologia LTK II\\n\\nMenu\\nSinfonia Quantistica delle Stelle\\nIngredienti\\n           Shard di Prisma Stellare\\n           Lattuga Namecciana\\n           Radici di SingolaritàFibra di Sintetex\\n           Carne di Balena spaziale\\n           Teste di Idra\\n           Nettare di Sirena\\n           Sale Temporale\\n\\nTecniche\\n           Marinatura Temporale Sincronizzata\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Cottura a Vapore Termocinetica Multipla\\n           Cottura a Vapore Ecodinamico Bilanciato'),\n",
      " Document(metadata={'FileName': 'Sapore del Dune.pdf'}, page_content=\"Sinfonia Quantistica dell'Universo\\nIngredienti\\n           Shard di Prisma Stellare\\n           Foglie di Nebulosa\\n           Lattuga Namecciana\\n           Teste di Idra\\n           Carne di Mucca\\n           Farina di Nettuno\\n           Riso di Cassandra\\n           Fusilli del Vento\\n           Nduja Fritta Tanto\\n\\nTecniche\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Cottura a Vapore Termocinetica Multipla\\n           Marinatura Temporale Sincronizzata\\n\\nEvanescenza Quantica\\nIngredienti\\n           Cristalli di Memoria\\n           Radici di Singolarità\\n           Lattuga Namecciana\\n           Baccacedro\\n           Granuli di Nebbia Arcobaleno\\n           Plasma Vitale\\n           Petali di Eco\"),\n",
      " Document(metadata={'FileName': 'Sapore del Dune.pdf'}, page_content='Tecniche\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Grigliatura Eletro-Molecolare a Spaziatura Variabile\\n           Bollitura Termografica a Rotazione Veloce\\n           Affumicatura a Stratificazione Quantica\\n           Saltare in Padella ClassicaGalassia di Sapori Sublimi\\nIngredienti\\n           Frammenti di Supernova\\n           Baccacedro\\n           Foglie di Nebulosa\\n           Radici di Singolarità\\n           Amido di Stellarion\\n           Fusilli del Vento\\n\\nTecniche\\n           Cottura con Microonde Entropiche Sincronizzate\\n           Congelamento Bio-Luminiscente Sincronico\\n           Cottura Sottovuoto Frugale Energeticamente Negativa\\n           Marinatura Temporale Sincronizzata\\n           Impasto Gravitazionale Vorticoso')]\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "# Possible improvements - future hypertuning of chunk_size and chunk_overlap to improve results and try different slitters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(doc_data)\n",
    "pprint(splits[0:6])\n",
    "pprint(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7af1cee183b0>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embd)\n",
    "vectorstore.save_local(\"faiss_index_100\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "@tool(\"Ricerca nel vectorstore\")\n",
    "def search(query: str):\n",
    "    \"\"\"Agent to search in vectorstore\n",
    "    Args:\n",
    "        query: query of user\n",
    "    \"\"\"\n",
    "    return retriever.invoke(query)\n",
    "\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_agent_action callback: AttributeError(\"'str' object has no attribute 'log'\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m chain \u001b[38;5;241m=\u001b[39m (prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m StrOutputParser())\n\u001b[1;32m     34\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39mchain, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, handle_parsing_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is today\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms date?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1624\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1624\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1633\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1634\u001b[0m         )\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1330\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1323\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1328\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1330\u001b[0m         \u001b[43m[\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1340\u001b[0m     )\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1415\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[0;32m-> 1415\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/langchain/agents/agent.py:1429\u001b[0m, in \u001b[0;36mAgentExecutor._perform_agent_action\u001b[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_agent_action(agent_action, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;66;03m# Otherwise we lookup the tool\u001b[39;00m\n\u001b[0;32m-> 1429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43magent_action\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool\u001b[49m \u001b[38;5;129;01min\u001b[39;00m name_to_tool_map:\n\u001b[1;32m   1430\u001b[0m     tool \u001b[38;5;241m=\u001b[39m name_to_tool_map[agent_action\u001b[38;5;241m.\u001b[39mtool]\n\u001b[1;32m   1431\u001b[0m     return_direct \u001b[38;5;241m=\u001b[39m tool\u001b[38;5;241m.\u001b[39mreturn_direct\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'tool'"
     ]
    }
   ],
   "source": [
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Rispondi alla domanda dell'utente il più accuratamente possibile. \n",
    "Ritorna solo i piatti che soddisfano le richieste\n",
    "usando i file estratti con il tool.\"\"\"\n",
    "\n",
    "human_prompt = \"\"\"{input}\n",
    "(rispondi sempre con una lista di piatti)\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = WatsonxLLM(\n",
    "    model_id=\"mistralai/mistral-large\",  # Che conosciamo bene 😊🏆\n",
    "    url=credentials.url,\n",
    "    apikey=credentials.api_key,\n",
    "    project_id=\"5c33debb-5a25-4bfe-8392-ede4b20884fe\",\n",
    "    params={\n",
    "        \"max_new_tokens\": 1024\n",
    "    }\n",
    ")\n",
    "\n",
    "# Now you can use llm_runnable in the chain\n",
    "chain = (prompt | llm | StrOutputParser())\n",
    "\n",
    "agent_executor = AgentExecutor(agent=chain, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What is today's date?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAssistantAgent\n",
    "\n",
    "agent = OpenAIAssistantAgent.from_new(\n",
    "    name=\"QA bot sulla galassia\",\n",
    "    instructions=\"Sei un assistente che mi aiuta a rispondere a domande sui piatti da mangiare delle galassia. Ritorna solo la lista dei nomi dei piatti di interesse\",\n",
    "    openai_tools=[],\n",
    "    tools=[summary_tool, vector_tool],\n",
    "    verbose=True,\n",
    "    run_retrieve_sleep_time=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool(\"Ricerca nel vectorstore\")\n",
    "def search(query: str):\n",
    "    \"\"\"Agent to search in vectorstore\n",
    "    Args:\n",
    "        query: query of user\n",
    "    \"\"\"\n",
    "    return retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mWatsonxLLM\u001b[0m\n",
      "Params: {'model_id': 'mistralai/mistral-large', 'deployment_id': None, 'params': {'max_new_tokens': 1024}, 'project_id': '5c33debb-5a25-4bfe-8392-ede4b20884fe', 'space_id': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mWatsonxLLM\u001b[0m\n",
      "Params: {'model_id': 'mistralai/mistral-large', 'deployment_id': None, 'params': {'max_new_tokens': 1024}, 'project_id': '5c33debb-5a25-4bfe-8392-ede4b20884fe', 'space_id': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "ERROR:root:Failed to get supported params: argument of type 'NoneType' is not iterable\n",
      "ERROR:root:LiteLLM call failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mWatsonxLLM\u001b[0m\n",
      "Params: {'model_id': 'mistralai/mistral-large', 'deployment_id': None, 'params': {'max_new_tokens': 1024}, 'project_id': '5c33debb-5a25-4bfe-8392-ede4b20884fe', 'space_id': None}\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mWatsonxLLM\u001b[0m\nParams: {'model_id': 'mistralai/mistral-large', 'deployment_id': None, 'params': {'max_new_tokens': 1024}, 'project_id': '5c33debb-5a25-4bfe-8392-ede4b20884fe', 'space_id': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agent.py:333\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_for_human_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 102\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:206\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_logs(formatted_answer)\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:115\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit():\n\u001b[0;32m--> 115\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/llm.py:181\u001b[0m, in \u001b[0;36mLLM.call\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    179\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 181\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/utils.py:1033\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1031\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1032\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/utils.py:909\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/main.py:2967\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2966\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 2967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   2968\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   2969\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   2970\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   2971\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2972\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   2973\u001b[0m     )\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/main.py:937\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    936\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 937\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hidden_params\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:356\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:333\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    335\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    336\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[1;32m    337\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m    338\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    339\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    340\u001b[0m         ),\n\u001b[1;32m    341\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mWatsonxLLM\u001b[0m\nParams: {'model_id': 'mistralai/mistral-large', 'deployment_id': None, 'params': {'max_new_tokens': 1024}, 'project_id': '5c33debb-5a25-4bfe-8392-ede4b20884fe', 'space_id': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agent.py:333\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_for_human_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 102\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:206\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_logs(formatted_answer)\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:115\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit():\n\u001b[0;32m--> 115\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/llm.py:181\u001b[0m, in \u001b[0;36mLLM.call\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    179\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 181\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/utils.py:1033\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1031\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1032\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/utils.py:909\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/main.py:2967\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2966\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 2967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   2968\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   2969\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   2970\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   2971\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2972\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   2973\u001b[0m     )\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/main.py:937\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    936\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 937\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hidden_params\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:356\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:333\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    335\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    336\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[1;32m    337\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m    338\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    339\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    340\u001b[0m         ),\n\u001b[1;32m    341\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mWatsonxLLM\u001b[0m\nParams: {'model_id': 'mistralai/mistral-large', 'deployment_id': None, 'params': {'max_new_tokens': 1024}, 'project_id': '5c33debb-5a25-4bfe-8392-ede4b20884fe', 'space_id': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 47\u001b[0m\n\u001b[1;32m     40\u001b[0m task \u001b[38;5;241m=\u001b[39m Task(\n\u001b[1;32m     41\u001b[0m     description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuali piatti hanno i ravioli Vaporeon?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     42\u001b[0m     expected_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUna lista di piatti che soddisfa la richiesta dell\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutente\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     43\u001b[0m     agent \u001b[38;5;241m=\u001b[39m researcher\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m crew \u001b[38;5;241m=\u001b[39m Crew(agent \u001b[38;5;241m=\u001b[39m [researcher], tasks \u001b[38;5;241m=\u001b[39m [task], verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, share_crew \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/crew.py:548\u001b[0m, in \u001b[0;36mCrew.kickoff\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    545\u001b[0m metrics: List[UsageMetrics] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39msequential:\n\u001b[0;32m--> 548\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess \u001b[38;5;241m==\u001b[39m Process\u001b[38;5;241m.\u001b[39mhierarchical:\n\u001b[1;32m    550\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_hierarchical_process()\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/crew.py:655\u001b[0m, in \u001b[0;36mCrew._run_sequential_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CrewOutput:\n\u001b[1;32m    654\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/crew.py:756\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[0;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[1;32m    753\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    755\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[0;32m--> 756\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_for_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m task_outputs \u001b[38;5;241m=\u001b[39m [task_output]\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/task.py:302\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    297\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    298\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    299\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/task.py:366\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[0;34m(self, agent, context, tools)\u001b[0m\n\u001b[1;32m    362\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[0;32m--> 366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[1;32m    373\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[1;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    375\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[1;32m    382\u001b[0m )\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agent.py:345\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller\u001b[38;5;241m.\u001b[39mstop_rpm_counter()\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agent.py:345\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller\u001b[38;5;241m.\u001b[39mstop_rpm_counter()\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agent.py:344\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_task(task, context, tools)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rpm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rpm_controller:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agent.py:333\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[0;34m(self, task, context, tools)\u001b[0m\n\u001b[1;32m    330\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_for_human_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_start_logs()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 102\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input:\n\u001b[1;32m    105\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_human_feedback(formatted_answer)\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:206\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_loop(formatted_answer)\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_logs(formatted_answer)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_answer\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py:115\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[0;34m(self, formatted_answer)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentFinish):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_within_rpm_limit():\n\u001b[0;32m--> 115\u001b[0m         answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    121\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[1;32m    122\u001b[0m                 content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m                 color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    124\u001b[0m             )\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/crewai/llm.py:181\u001b[0m, in \u001b[0;36mLLM.call\u001b[0;34m(self, messages, callbacks)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# Remove None values to avoid passing unnecessary parameters\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m--> 181\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/utils.py:1033\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1030\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1031\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1032\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/utils.py:909\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/main.py:2967\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   2965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2966\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 2967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[1;32m   2968\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   2969\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[1;32m   2970\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   2971\u001b[0m         completion_kwargs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2972\u001b[0m         extra_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   2973\u001b[0m     )\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/main.py:937\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     model \u001b[38;5;241m=\u001b[39m deployment_id\n\u001b[1;32m    936\u001b[0m     custom_llm_provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 937\u001b[0m model, custom_llm_provider, dynamic_api_key, api_base \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hidden_params\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    944\u001b[0m     model_response\u001b[38;5;241m.\u001b[39m_hidden_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_llm_provider\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m custom_llm_provider\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:356\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError):\n\u001b[0;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         )\n",
      "File \u001b[0;32m~/VSCodeProjects/PythonProjects/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/get_llm_provider_logic.py:333\u001b[0m, in \u001b[0;36mget_llm_provider\u001b[0;34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[0m\n\u001b[1;32m    331\u001b[0m     error_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Pass model as E.g. For \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuggingface\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m inference endpoints pass in `completion(model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuggingface/starcoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         message\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    335\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    336\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[1;32m    337\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m    338\u001b[0m             content\u001b[38;5;241m=\u001b[39merror_str,\n\u001b[1;32m    339\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    340\u001b[0m         ),\n\u001b[1;32m    341\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(api_base)\n\u001b[1;32m    346\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[1mWatsonxLLM\u001b[0m\nParams: {'model_id': 'mistralai/mistral-large', 'deployment_id': None, 'params': {'max_new_tokens': 1024}, 'project_id': '5c33debb-5a25-4bfe-8392-ede4b20884fe', 'space_id': None}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "from crewai import Crew, Task, Agent\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    api_key=\"AnzfgthfcrfRzttoXGiKZUJDMRlcB3w4uemf0PJGFFT5\"\n",
    ")\n",
    "\n",
    "OVERWRITE = False\n",
    "\n",
    "llm = WatsonxLLM(\n",
    "    model_id=\"mistralai/mistral-large\",  # Che conosciamo bene 😊🏆\n",
    "    url=credentials.url,\n",
    "    apikey=credentials.api_key,\n",
    "    project_id=\"5c33debb-5a25-4bfe-8392-ede4b20884fe\",\n",
    "    params={\n",
    "        \"max_new_tokens\": 1024\n",
    "    }\n",
    ")\n",
    "\n",
    "function_calling_llm = WatsonxLLM(\n",
    "    model_id=\"mistralai/mistral-large\",  # Che conosciamo bene 😊🏆\n",
    "    url=credentials.url,\n",
    "    apikey=credentials.api_key,\n",
    "    project_id=\"5c33debb-5a25-4bfe-8392-ede4b20884fe\",\n",
    "    params={\n",
    "        \"max_new_tokens\": 1024\n",
    "    }\n",
    ")\n",
    "\n",
    "researcher = Agent(\n",
    "    llm = llm,\n",
    "    function_calling_llm = function_calling_llm,\n",
    "    role=\"QA bot sulla galassia\",\n",
    "    goal = \"Rispondere alla domanda dell'utente con una lista di piatti che soddisfano la richiesta\",\n",
    "    backstory=\"Sei un assistente che mi aiuta a rispondere a domande sui piatti da mangiare delle galassia. Ritorna solo la lista dei nomi dei piatti di interesse\",\n",
    "    tools=[search]\n",
    ")\n",
    "\n",
    "task = Task(\n",
    "    description = \"Quali piatti hanno i ravioli Vaporeon?\", \n",
    "    expected_output = \"Una lista di piatti che soddisfa la richiesta dell'utente\", \n",
    "    agent = researcher\n",
    ")\n",
    "\n",
    "crew = Crew(agent = [researcher], tasks = [task], verbose = True, share_crew =  False)\n",
    "crew.kickoff()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"input\":\"Quali piatti contengono Ravioli Vaporeon?\"}\n",
      "Got output: I piatti che contengono i Ravioli Vaporeon sono \"Nebulae Di-Cedri Risvegliati\" e \"Nebulosa dell'Infinito\".\n",
      "========================\n",
      "- Nebulae Di-Cedri Risvegliati\n",
      "- Nebulosa dell'Infinito\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Quali piatti contengono Ravioli Vaporeon?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- Nebulae Di-Cedri Risvegliati\\n- Nebulosa dell'Infinito\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
